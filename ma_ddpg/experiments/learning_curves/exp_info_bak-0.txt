D:\Anaconda3\envs\tf1.14\python.exe D:/0_Desktop/研二下/组会/CODE/Multiple_UEs_UAVs_BSs/ue_uav_bs_212/MADDPG/experiments/train.py
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Please read the raised warning, then press Enter to continue... (to suppress this prompt, please set the environment variable `SUPPRESS_MA_PROMPT=1`)
d:\0_desktop\研二下\组会\code\code-from-network-for-maddpg\tensorflow_practice\multiagent-particle-envs\multiagent\__init__.py:23: UserWarning: This code base is no longer maintained, and is not expected to be maintained again in the future. 
For the past handful of years, these environments been maintained inside of PettingZoo (see https://pettingzoo.farama.org/environments/mpe/). 
This maintained version includes documentation, support for the PettingZoo API, support for current versions of Python, numerous bug fixes, 
support for installation via pip, and numerous other large quality of life improvements. 
We encourage researchers to switch to this maintained version for all purposes other than comparing to results run on this version of the environments. 

  warnings.warn("This code base is no longer maintained, and is not expected to be maintained again in the future. \n"

WARNING:tensorflow:From D:\0_Desktop\研二下\组会\CODE\Multiple_UEs_UAVs_BSs\ue_uav_bs_212\MADDPG\maddpg\common\tf_util.py:163: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From D:\0_Desktop\研二下\组会\CODE\Multiple_UEs_UAVs_BSs\ue_uav_bs_212\MADDPG\maddpg\common\tf_util.py:166: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2023-02-02 19:17:11.538210: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2023-02-02 19:17:11.538733: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2023-02-02 19:17:11.569869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56
pciBusID: 0000:01:00.0
2023-02-02 19:17:11.570173: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2023-02-02 19:17:11.570452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2023-02-02 19:17:12.134671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-02 19:17:12.134892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2023-02-02 19:17:12.135038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2023-02-02 19:17:12.135363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2927 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)
WARNING:tensorflow:From D:\0_Desktop\研二下\组会\CODE\Multiple_UEs_UAVs_BSs\ue_uav_bs_212\MADDPG\maddpg\common\tf_util.py:84: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From D:\0_Desktop\研二下\组会\CODE\Multiple_UEs_UAVs_BSs\ue_uav_bs_212\MADDPG\maddpg\trainer\maddpg_renamed.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From D:\0_Desktop\研二下\组会\CODE\Multiple_UEs_UAVs_BSs\ue_uav_bs_212\MADDPG\maddpg\trainer\maddpg_renamed.py:76: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From D:\0_Desktop\研二下\组会\CODE\Multiple_UEs_UAVs_BSs\ue_uav_bs_212\MADDPG\maddpg\common\tf_util.py:214: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

WARNING:tensorflow:From D:\Anaconda3\envs\tf1.14\lib\site-packages\tensorflow\python\ops\clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Using good policy maddpg and adv policy maddpg
WARNING:tensorflow:From D:/0_Desktop/研二下/组会/CODE/Multiple_UEs_UAVs_BSs/ue_uav_bs_212/MADDPG/experiments/train.py:110: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

Starting iterations...
down
down
down
down
down
down
down
down
down
steps: 1251, episodes: 10, mean episode reward: -63.10025043943231, time: 2.879
down
down
down
down
down
down
down
down
down
down
steps: 2720, episodes: 20, mean episode reward: -60.96508898124947, time: 3.081
down
down
down
down
down
down
down
down
down
down
steps: 4188, episodes: 30, mean episode reward: -63.901569163498245, time: 3.02
down
down
down
down
down
down
down
down
down
down
steps: 5657, episodes: 40, mean episode reward: -57.062767601041536, time: 3.106
down
down
down
down
down
down
down
down
down
down
steps: 7108, episodes: 50, mean episode reward: -63.70906666681682, time: 2.909
down
down
down
down
down
down
down
down
down
down
steps: 8608, episodes: 60, mean episode reward: -61.04102972384527, time: 3.077
down
down
down
down
down
down
down
down
down
down
steps: 10083, episodes: 70, mean episode reward: -62.81512523960231, time: 2.986
down
down
down
down
down
down
down
down
down
down
steps: 11610, episodes: 80, mean episode reward: -62.60222185007093, time: 3.151
down
down
down
down
down
down
down
down
down
down
steps: 13111, episodes: 90, mean episode reward: -58.559720968427584, time: 3.088
down
down
down
down
down
down
down
down
down
down
steps: 14579, episodes: 100, mean episode reward: -62.87158817554509, time: 2.921
down
down
down
down
down
down
down
down
down
down
steps: 16043, episodes: 110, mean episode reward: -64.47876752319962, time: 3.154
down
down
down
down
down
steps: 19943, episodes: 120, mean episode reward: 5.534303115514492, time: 7.855
down
down
down
go_away
go_away
steps: 23788, episodes: 130, mean episode reward: -11.041565637382831, time: 7.606
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 27406, episodes: 140, mean episode reward: -7.538775772745137, time: 8.142
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 29380, episodes: 150, mean episode reward: -51.44593480514497, time: 5.4
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 32291, episodes: 160, mean episode reward: -48.78226932408599, time: 9.061
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 34465, episodes: 170, mean episode reward: -60.28757342287901, time: 4.729
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 35726, episodes: 180, mean episode reward: -72.97604991177967, time: 2.751
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 37355, episodes: 190, mean episode reward: -80.63906280967394, time: 3.232
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 39891, episodes: 200, mean episode reward: -70.3280268407236, time: 4.9
go_away
go_away
go_away
go_away
steps: 42695, episodes: 210, mean episode reward: -33.903626415000495, time: 6.19
go_away
go_away
go_away
go_away
go_away
steps: 45829, episodes: 220, mean episode reward: -79.20314589606907, time: 11.856
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 46467, episodes: 230, mean episode reward: -85.38373682143944, time: 2.483
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 46730, episodes: 240, mean episode reward: -85.36136423397191, time: 0.854
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 46994, episodes: 250, mean episode reward: -85.36885479744929, time: 0.858
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 47255, episodes: 260, mean episode reward: -85.38304189502337, time: 0.847
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 47522, episodes: 270, mean episode reward: -85.28047557542854, time: 0.868
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 47796, episodes: 280, mean episode reward: -85.12513843221163, time: 0.888
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 48077, episodes: 290, mean episode reward: -84.96355038058564, time: 0.909
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 48361, episodes: 300, mean episode reward: -84.78706719127374, time: 0.918
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 48658, episodes: 310, mean episode reward: -84.80924454155546, time: 0.981
go_away
go_away
down
go_away
go_away
go_away
go_away
go_away
go_away
go_away
steps: 49245, episodes: 320, mean episode reward: -88.14490634451965, time: 2.115
go_away
go_away
down
down
down
down
down
down
down
steps: 51260, episodes: 330, mean episode reward: -33.00253638944314, time: 6.164
down
down
down
down
down
down
down
down
down
steps: 53243, episodes: 340, mean episode reward: -33.63388174286751, time: 4.511
go_away
go_away
go_away
go_away
go_away
go_away
go_away
down
down
down
steps: 54988, episodes: 350, mean episode reward: -85.72843498256398, time: 3.531
down
down
down
down
down
down
steps: 57557, episodes: 360, mean episode reward: 7.112936658483937, time: 5.904
down
down
steps: 61017, episodes: 370, mean episode reward: 90.4182861778716, time: 8.841
down
down
steps: 64773, episodes: 380, mean episode reward: 61.53884401952995, time: 9.915
down
down
steps: 68789, episodes: 390, mean episode reward: 60.5778728988419, time: 9.719
down
#############################################################################################
down
steps: 72479, episodes: 400, mean episode reward: 65.58809818920791, time: 8.431
down
steps: 76697, episodes: 410, mean episode reward: 9.998886776066575, time: 9.415
down
go_away
go_away
down
down
steps: 79998, episodes: 420, mean episode reward: -50.98972919755706, time: 6.755
down
steps: 84024, episodes: 430, mean episode reward: 60.93299025406321, time: 9.054
steps: 88657, episodes: 440, mean episode reward: 89.48977167484432, time: 11.228
#############################################################################################
down
steps: 93381, episodes: 450, mean episode reward: 147.45837829394003, time: 12.519
#############################################################################################
steps: 98255, episodes: 460, mean episode reward: 173.2324018004313, time: 12.899
no_energy
#############################################################################################
#############################################################################################
#############################################################################################
#############################################################################################
steps: 103162, episodes: 470, mean episode reward: 188.29419321813378, time: 13.061
#############################################################################################
#############################################################################################
#############################################################################################
steps: 107991, episodes: 480, mean episode reward: 224.32850020402762, time: 12.018
#############################################################################################
#############################################################################################
#############################################################################################
steps: 112995, episodes: 490, mean episode reward: 208.7473532300991, time: 13.518
#############################################################################################
steps: 117826, episodes: 500, mean episode reward: 160.54222472170483, time: 12.89
...Finished total of 501 episodes.

Process finished with exit code 0
